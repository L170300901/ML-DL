{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Additional Functions with Keras\n",
    "\n",
    "- 3.1 모델 합치기 with CIFAR10\n",
    "- 3.2 데이터 증강\n",
    "- 3.3 Finetuning\n",
    "- 3.4 LSTM with MNIST\n",
    "- 3.5 GAN with MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 모델 합치기 with CIFAR10\n",
    "\n",
    "convolution과 maxpooling layer로 구성된 feature extractor 모델과\n",
    "\n",
    "fully connected layer로 구성된 ANN classifier 모델을 따로 정의하고\n",
    "\n",
    "두 모델을 합쳐서 CNN 모델을 만듬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) 데이터셋 : CIFAR 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.utils as utils\n",
    "from tensorflow.keras import datasets\n",
    "\n",
    "# Dataset Load\n",
    "(X_train, Y_train), (X_test, Y_test) = datasets.cifar10.load_data()\n",
    "\n",
    "# Dataset Confirm\n",
    "print(X_train.shape, Y_train.shape)\n",
    "print('label : ',Y_train[0])\n",
    "plt.imshow(X_train[0])\n",
    "\n",
    "# Dataset Preprocessing\n",
    "X_train = X_train/255.0\n",
    "X_test = X_test/255.0\n",
    "Y_train = utils.to_categorical(Y_train)\n",
    "Y_test = utils.to_categorical(Y_test)\n",
    "\n",
    "print(X_train.shape, Y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) 모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Input, Dense, Activation\n",
    "from tensorflow.keras.layers import Flatten, BatchNormalization, Dropout, ReLU\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_in = X_train.shape[1:]\n",
    "n_out = Y_train.shape[-1]\n",
    "\n",
    "def conv_maxpool_layers(n_in):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(16, kernel_size=(3, 3), padding='same', activation='relu', input_shape=(n_in)))\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3), padding='same', strides=(2, 2), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def fc_layers(n_out):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units =128, input_shape=(2048,), activation='relu'))\n",
    "    model.add(Dense(units =n_out, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def CNN_sum(n_in, n_out):\n",
    "    \n",
    "    # Coding Time\n",
    "    \n",
    "    #각 부분 모델\n",
    "\n",
    "    \n",
    "    #두 모델을 합쳐 새로운 모델 정의(Functional Style)\n",
    "\n",
    "    \n",
    "    '''\n",
    "    Sequential Style\n",
    "    model = Sequential()\n",
    "    model.add(feature_extractor)\n",
    "    model.add(ann_classifier)\n",
    "    '''\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN_sum(n_in, n_out)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3-4) 모델의 학습과정 설정 / 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "earlystopper = EarlyStopping(monitor='val_accuracy', patience=7, verbose=1, mode='auto', restore_best_weights=True)\n",
    "history = model.fit(X_train, Y_train, batch_size=128, epochs=50, validation_split=0.2, callbacks = [earlystopper])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5) 모델 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_and_accuracy = model.evaluate(X_test, Y_test, batch_size=128)\n",
    "print('loss : %.4f, accruracy : %.4f'%(loss_and_accuracy[0],loss_and_accuracy[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Image data augmentation\n",
    "케라스에서는 ImageDataGenerate 함수로 Data augmentation 기능을  제공\n",
    "\n",
    "https://keras.io/preprocessing/image/#imagedatagenerator-class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1)-2 데이터 증강 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center = False,\n",
    "    samplewise_center = False,\n",
    "    featurewise_std_normalization = False,\n",
    "    samplewise_std_normalization = False,\n",
    "    zca_whitening = False,\n",
    "    rotation_range = 2, # 회전\n",
    "    zoom_range = 0.1, # 확대 축소\n",
    "    width_shift_range = 0.1, # 수평 이동\n",
    "    height_shift_range = 0.1, # 수직 이동\n",
    "    horizontal_flip = True, # 수평 반전|\n",
    "    vertical_flip = False # 수직 반전\n",
    ")\n",
    "\n",
    "datagen.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### (3-4) 모델의 학습과정 설정 / 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = CNN_sum(n_in, n_out)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "earlystopper = EarlyStopping(monitor='val_accuracy', patience=7, verbose=1, mode='auto', restore_best_weights=True)\n",
    "# Coding Time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_and_accuracy = model.evaluate(X_test, Y_test, batch_size=128)\n",
    "print('loss : %.4f, accruracy : %.4f'%(loss_and_accuracy[0],loss_and_accuracy[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Transfer learning\n",
    "Transfer learning을 통해 현재 쓰이고 있는 네트워크를 가져와 학습하는 방법을 배워본다(Classifier만 / Entire)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) 모델링1 : Classifier learning\n",
    "    \n",
    "사용가능 네트워크 :\n",
    "https://keras.io/api/applications/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coding Time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coding Time\n",
    "# Get Feature Extroactor from VGG16\n",
    "\n",
    "# Add Classifier\n",
    "\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first: train only the top layers (which were randomly initialized)\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) 모델의 학습과정 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) 모델 학습시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "earlystopper = EarlyStopping(monitor='val_accuracy', patience=7, verbose=1, mode='auto', restore_best_weights=True)\n",
    "history = model.fit(X_train, Y_train, batch_size=128, epochs=50, validation_split=0.2, callbacks = [earlystopper])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, loss_ax = plt.subplots()\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "loss_ax.plot(history.history['loss'], 'y', label='train loss')\n",
    "loss_ax.plot(history.history['val_loss'], 'r', label='val loss')\n",
    "\n",
    "acc_ax.plot(history.history['accuracy'], 'b', label='train acc')\n",
    "acc_ax.plot(history.history['val_accuracy'], 'g', label='val acc')\n",
    "\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "acc_ax.set_ylabel('accuray')\n",
    "\n",
    "loss_ax.legend(loc='upper left')\n",
    "acc_ax.legend(loc='lower left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5) 모델 평가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss_and_accuracy = model.evaluate(X_test, Y_test, batch_size=128)\n",
    "print('loss : %.4f, accruracy : %.4f'%(loss_and_accuracy[0],loss_and_accuracy[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) 모델링1 : Entire\n",
    "    \n",
    "사용가능 네트워크 :\n",
    "https://keras.io/api/applications/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Feature Extroactor from VGG16\n",
    "x = base_model.output\n",
    "\n",
    "# Add Classifier\n",
    "x = Flatten()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(256)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = ReLU()(x)\n",
    "predictions = Dense(Y_train.shape[1], activation='softmax')(x) #Y_train.shape[1] :10 \n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first: train the all layers (which were randomly initialized)\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) 모델의 학습과정 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) 모델 학습시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "earlystopper = EarlyStopping(monitor='val_accuracy', patience=7, verbose=1, mode='auto', restore_best_weights=True)\n",
    "history = model.fit(X_train, Y_train, batch_size=128, epochs=50, validation_split=0.2, callbacks = [earlystopper])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, loss_ax = plt.subplots()\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "loss_ax.plot(history.history['loss'], 'y', label='train loss')\n",
    "loss_ax.plot(history.history['val_loss'], 'r', label='val loss')\n",
    "\n",
    "acc_ax.plot(history.history['accuracy'], 'b', label='train acc')\n",
    "acc_ax.plot(history.history['val_accuracy'], 'g', label='val acc')\n",
    "\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "acc_ax.set_ylabel('accuray')\n",
    "\n",
    "loss_ax.legend(loc='upper left')\n",
    "acc_ax.legend(loc='lower left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5) 모델 평가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_and_accuracy = model.evaluate(X_test, Y_test, batch_size=128)\n",
    "print('loss : %.4f, accruracy : %.4f'%(loss_and_accuracy[0],loss_and_accuracy[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 LSTM with MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) 데이터셋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.utils as utils\n",
    "from tensorflow.keras import datasets\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(X_train, Y_train),(X_test, Y_test) = datasets.mnist.load_data()\n",
    "X_train_norm = X_train.astype('float32')/255.0\n",
    "X_test_norm = X_test.astype('float32')/255.0\n",
    "Y_train_onehot = utils.to_categorical(Y_train)\n",
    "Y_test_onehot = utils.to_categorical(Y_test)\n",
    "\n",
    "print(X_train_norm.shape, Y_train_onehot.shape)\n",
    "n_in = X_train.shape[1:]\n",
    "n_out = Y_train_onehot.shape[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) 모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm(n_in, n_out):\n",
    "    # Coding Time\n",
    "\n",
    "    return model\n",
    "\n",
    "model = lstm(n_in, n_out)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) 모델의 학습과정 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Coding Time\n",
    "earlystopper = EarlyStopping(monitor='val_accuracy', patience=7, verbose=1, mode='auto', restore_best_weights=True)\n",
    "history = model.fit(X_train_norm, Y_train_onehot, batch_size=128, epochs=50, validation_split=0.2, callbacks = [earlystopper])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, loss_ax = plt.subplots()\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "loss_ax.plot(history.history['loss'], 'y', label='train loss')\n",
    "loss_ax.plot(history.history['val_loss'], 'r', label='val loss')\n",
    "\n",
    "acc_ax.plot(history.history['accuracy'], 'b', label='train acc')\n",
    "acc_ax.plot(history.history['val_accuracy'], 'g', label='val acc')\n",
    "\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "acc_ax.set_ylabel('accuray')\n",
    "\n",
    "loss_ax.legend(loc='upper left')\n",
    "acc_ax.legend(loc='lower left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5) 모델 평가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss_and_accuracy = model.evaluate(X_test_norm, Y_test_onehot, batch_size=128, verbose=1)\n",
    "print('loss : %.4f, accruracy : %.4f'%(loss_and_accuracy[0], loss_and_accuracy[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 GAN with MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Dropout, LeakyReLU\n",
    "from keras.models import Model, Sequential\n",
    "from keras.datasets import mnist\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras import initializers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실험을 재현하고 동일한 결과를 얻을 수 있는지 확인하기 위해 seed 를 설정합니다.\n",
    "np.random.seed(10)\n",
    "\n",
    "# 우리의 랜덤 노이즈 벡터의 차원을 설정합니다.\n",
    "random_dim = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) 데이터셋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_minst_data():\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    x_train = (x_train.astype(np.float32) - 127.5)/127.5 # -1~1 사이의 값\n",
    "    x_train = x_train.reshape(60000, 784)\n",
    "    return (x_train, y_train, x_test, y_test) # Label에 대한 진행이 없어도 됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2-3) 모델링 / 모델 학습과정 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam Optimizer를 사용합니다.\n",
    "def get_optimizer():\n",
    "    return Adam(learning_rate=0.0002, beta_1=0.5)\n",
    "\n",
    "# Generator 만들기\n",
    "def get_generator(optimizer):\n",
    "    generator = Sequential()\n",
    "    generator.add(Dense(256, input_dim=random_dim, kernel_initializer=initializers.RandomNormal(stddev=0.02)))\n",
    "    generator.add(LeakyReLU(0.2))\n",
    "\n",
    "    generator.add(Dense(512))\n",
    "    generator.add(LeakyReLU(0.2))\n",
    "\n",
    "    generator.add(Dense(1024))\n",
    "    generator.add(LeakyReLU(0.2))\n",
    "\n",
    "    generator.add(Dense(784, activation='tanh'))\n",
    "    generator.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "    return generator\n",
    "\n",
    "# Discriminator 만들기\n",
    "def get_discriminator(optimizer):\n",
    "    discriminator = Sequential()\n",
    "    discriminator.add(Dense(1024, input_dim=784, kernel_initializer=initializers.RandomNormal(stddev=0.02)))\n",
    "    discriminator.add(LeakyReLU(0.2))\n",
    "    discriminator.add(Dropout(0.3))\n",
    "\n",
    "    discriminator.add(Dense(512))\n",
    "    discriminator.add(LeakyReLU(0.2))\n",
    "    discriminator.add(Dropout(0.3))\n",
    "\n",
    "    discriminator.add(Dense(256))\n",
    "    discriminator.add(LeakyReLU(0.2))\n",
    "    discriminator.add(Dropout(0.3))\n",
    "\n",
    "    discriminator.add(Dense(1, activation='sigmoid'))\n",
    "    discriminator.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "    return discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gan_network(discriminator, random_dim, generator, optimizer):\n",
    "    # Coding Time\n",
    "\n",
    "    gan.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "    return gan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 생성된 MNIST 이미지 출력\n",
    "def plot_generated_images(epoch, generator, examples=100, dim=(10, 10), figsize=(10, 10)):\n",
    "    noise = np.random.normal(0, 1, size=[examples, random_dim])\n",
    "    generated_images = generator.predict(noise)\n",
    "    generated_images = generated_images.reshape(examples, 28, 28)\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    for i in range(generated_images.shape[0]):\n",
    "        plt.subplot(dim[0], dim[1], i+1)\n",
    "        plt.imshow(generated_images[i], interpolation='nearest', cmap='gray_r')\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('gan_generated_image_epoch_%d.png' % epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs=1, batch_size=128):\n",
    "    # train 데이터와 test 데이터를 가져옵니다.\n",
    "    x_train, y_train, x_test, y_test = load_minst_data()\n",
    "\n",
    "    # train 데이터를 128 사이즈의 batch 로 나눕니다.\n",
    "    batch_count = x_train.shape[0] // batch_size\n",
    "\n",
    "    # 우리의 GAN 네트워크를 만듭니다.\n",
    "    adam = get_optimizer()\n",
    "    generator = get_generator(adam)\n",
    "    discriminator = get_discriminator(adam)\n",
    "    gan = get_gan_network(discriminator, random_dim, generator, adam)\n",
    "\n",
    "    for e in range(1, epochs+1):\n",
    "        print('-'*15, 'Epoch %d' % e, '-'*15)\n",
    "        for _ in tqdm(range(batch_count)):\n",
    "            # Coding Time\n",
    "            # 입력으로 사용할 random 노이즈와 이미지를 가져옵니다.\n",
    "\n",
    "\n",
    "            # Generator를 통해 MNIST 이미지를 생성\n",
    "\n",
    "\n",
    "            # Discriminator 학습\n",
    "\n",
    "\n",
    "            # Generator 학습\n",
    "\n",
    "        if e == 1 or e % 20 == 0:\n",
    "            plot_generated_images(e, generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train(20, 128)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mldl_0",
   "language": "python",
   "name": "mldl_0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
